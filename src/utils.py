# src/utils.py
# (V5.8 - ä¿®å¤ lxml æŠ¥é”™ï¼Œæ”¹ç”¨å†…ç½®è§£æå™¨)

import re
import nltk
import logging
import hashlib
import pandas as pd
from bs4 import BeautifulSoup
from pathlib import Path

# å¯¼å…¥é…ç½®
import settings

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


def setup_nltk():
    """
    åˆå§‹åŒ– NLTKï¼Œç¡®ä¿ punkt åˆ†è¯å™¨å¯ç”¨ã€‚
    """
    try:
        nltk.data.find('tokenizers/punkt')
        nltk.data.find('tokenizers/punkt_tab')
    except LookupError:
        logger.info("Downloading NLTK punkt tokenizer...")
        nltk.download('punkt')
        nltk.download('punkt_tab')


def load_cik_map(file_path):
    """
    åŠ è½½ CIK æ˜ å°„è¡¨ (Ticker -> CIK)ã€‚
    """
    try:
        if not Path(file_path).exists():
            logger.error(f"CIK map file not found: {file_path}")
            return pd.DataFrame()

        # è¯»å– CSVï¼Œå¼ºåˆ¶ CIK ä¸ºå­—ç¬¦ä¸²ä»¥ä¿ç•™å‰å¯¼é›¶
        df = pd.read_csv(file_path, dtype={'CIK': str})
        return df
    except Exception as e:
        logger.error(f"Error loading CIK map: {e}")
        return pd.DataFrame()


def get_document_text(file_path):
    """
    è¯»å–æ–‡ä»¶å¹¶æ¸…ç† HTML æ ‡ç­¾ (å¦‚æœå­˜åœ¨)ã€‚
    """
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()

        # ç®€å•çš„ HTML æ ‡ç­¾æ¸…æ´—
        if "<HTML>" in content.upper() or "<?XML" in content.upper():
            # ğŸ”´ ä¿®å¤ï¼šæ”¹ç”¨ 'html.parser' (Pythonå†…ç½®)ï¼Œé¿å… lxml ç¼ºå¤±æŠ¥é”™
            soup = BeautifulSoup(content, 'html.parser')
            text = soup.get_text(separator=' ', strip=True)
            return text
        else:
            return content
    except Exception as e:
        logger.error(f"Error reading file {file_path}: {e}")
        return ""


def extract_effective_date(text):
    """
    æå–è´·æ¬¾ç”Ÿæ•ˆæ—¥æœŸï¼Œæå–ä¸åˆ°è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
    """
    # æˆªå–å¤´éƒ¨ä»¥æé«˜æ•ˆç‡
    header_text = text[:settings.HEADER_ONLY_CHAR_LIMIT]

    match = settings.DATE_REGEX.search(header_text)
    if match:
        return match.group(1).strip()
    return ""


def find_supply_chain_sentences(text: str) -> list[str]:
    """
    åˆ†å¥å¹¶ç­›é€‰åŒ…å«ä¾›åº”é“¾ä¿¡æ¯çš„å¥å­ã€‚

    ã€æ ¸å¿ƒé€»è¾‘ V5.6ã€‘
    å¥å­ä¿ç•™çš„æ¡ä»¶æ˜¯ï¼š
    1. åŒ…å«æ ¸å¿ƒè¯ (CORE_KEYWORDS_REGEX) -> å¦‚ "supply chain"
       OR
    2. åŒæ—¶åŒ…å«å®ä½“è¯ (CONTEXT_KEYWORDS_REGEX) AND è¿è¥/å…³ç³»è¯ (OPERATIONAL_CONTEXT_REGEX)
       -> å¦‚ "maintain relationship" (è¿è¥) + "with suppliers" (å®ä½“)
    """
    if not text:
        return []

    # 1. NLTK åˆ†å¥
    try:
        sentences = nltk.sent_tokenize(text)
    except Exception:
        # å›é€€æ–¹æ¡ˆ
        sentences = text.split('. ')

    valid_sentences = []
    seen_hashes = set()  # å•æ–‡ä»¶å†…å»é‡

    for sent in sentences:
        # æ¸…æ´—ç©ºç™½å­—ç¬¦
        sent_clean = sent.strip().replace('\n', ' ')

        # åŸºç¡€é•¿åº¦è¿‡æ»¤
        if len(sent_clean) < settings.MIN_SENTENCE_LENGTH:
            continue

        # --- ğŸ” ç­›é€‰é€»è¾‘æ ¸å¿ƒ ---

        # A. æ ¸å¿ƒè¯ç›´æ¥å‘½ä¸­
        has_core = bool(settings.CORE_KEYWORDS_REGEX.search(sent_clean))

        # B. å®ä½“è¯ + è¿è¥è¯ ç»„åˆå‘½ä¸­
        has_context = bool(settings.CONTEXT_KEYWORDS_REGEX.search(sent_clean))
        has_operational = bool(settings.OPERATIONAL_CONTEXT_REGEX.search(sent_clean))

        is_relevant = has_core or (has_context and has_operational)

        if is_relevant:
            # --- ğŸš« å™ªéŸ³è¿‡æ»¤ ---
            if settings.NOISE_LEGAL_JARGON_REGEX.search(sent_clean):
                continue
            if settings.NOISE_TABLE_REGEX.search(sent_clean):
                continue
            if settings.NOISE_LIST_REGEX.search(sent_clean):
                continue
            if settings.NOISE_DOTS_REGEX.search(sent_clean):
                continue
            if settings.NOISE_PAGE_NUMBER_REGEX.search(sent_clean):
                continue
            if settings.NOISE_GARBAGE_REGEX.search(sent_clean):
                continue
            if settings.NOISE_DEFINITION_REGEX.search(sent_clean):
                continue

            # --- ğŸ”’ æŒ‡çº¹å»é‡ ---
            sent_hash = hashlib.md5(sent_clean.encode('utf-8')).hexdigest()

            if sent_hash not in seen_hashes:
                seen_hashes.add(sent_hash)
                valid_sentences.append(sent_clean)

    return valid_sentences